{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "torch.manual_seed(69)\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 128\n",
    "TEST_SUBSET_SIZE = 5000\n",
    "LEARNING_RATE = 1e-4\n",
    "IMAGE_SIZE = 128\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 256\n",
    "NUM_WORKERS = 2\n",
    "LOG_FOLDER = \"logs/\"\n",
    "os.makedirs(LOG_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generators\n",
    "fixed_test_vector = torch.randn(TEST_SUBSET_SIZE//BATCH_SIZE, BATCH_SIZE, Z_DIM, 1, 1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_file, model, device='cuda'):\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    print(\"=> Loaded checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(discriminator, generator, test_loader, fixed_test_vector, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluates a discriminator on both real and generated samples.\n",
    "\n",
    "    Args:\n",
    "        discriminator: Discriminator model in eval mode\n",
    "        generator: Generator model\n",
    "        test_loader: DataLoader for real samples\n",
    "        fixed_test_vector: Latent vectors for generating fake samples\n",
    "        device: Device to run evaluation on\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    running_loss = 0.0\n",
    "    preds, scores, targets = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(test_loader, leave=True, desc=\"Testing\")\n",
    "        for batch_idx, (real, real_labels) in enumerate(loop):\n",
    "            real = real.to(device)\n",
    "            real_labels = real_labels.to(device).float()\n",
    "            real_outputs = discriminator(real).squeeze()\n",
    "            real_loss = criterion(real_outputs, real_labels)\n",
    "\n",
    "            fake = generator(fixed_test_vector[batch_idx])\n",
    "            fake_labels = torch.zeros_like(real_labels).to(device).float()\n",
    "            fake_outputs = discriminator(fake).squeeze()\n",
    "            fake_loss = criterion(fake_outputs, fake_labels)\n",
    "\n",
    "            loss = real_loss + fake_loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            outputs = torch.cat((real_outputs, fake_outputs), dim=0).cpu().numpy()\n",
    "            labels = torch.cat((real_labels, fake_labels), dim=0).cpu().numpy()\n",
    "            scores = np.concatenate((scores, outputs))\n",
    "            targets = np.concatenate((targets, labels))\n",
    "            preds = (scores >= 0.5).astype(int) # Converting to discreet 1s and 0s instead of probability\n",
    "            metrics = {\n",
    "                'loss': running_loss / (batch_idx + 1),\n",
    "                'accuracy': accuracy_score(targets, preds),\n",
    "                'precision': precision_score(targets, preds, zero_division=1),\n",
    "                'recall': recall_score(targets, preds, zero_division=1),\n",
    "                'f1': f1_score(targets, preds, zero_division=1),\n",
    "                'roc_auc': roc_auc_score(targets, scores)\n",
    "            }\n",
    "\n",
    "            loop.set_postfix(**metrics)\n",
    "\n",
    "    final_metrics = {\n",
    "        k: v/len(test_loader) if k == 'loss' else v\n",
    "        for k, v in metrics.items()\n",
    "    }\n",
    "\n",
    "    print(f\"Test: Loss: {final_metrics['loss']:.4f} | \"\n",
    "          f\"Accuracy: {final_metrics['accuracy']:.4f} | \"\n",
    "          f\"Precision: {final_metrics['precision']:.4f} | \"\n",
    "          f\"Recall: {final_metrics['recall']:.4f} | \"\n",
    "          f\"F1: {final_metrics['f1']:.4f} | \"\n",
    "          f\"ROC AUC: {final_metrics['roc_auc']:.4f}\")\n",
    "\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PRO_CRITIC = \"models/pro_critic_128_2.pth\"\n",
    "CHECKPOINT_PRO_GEN = \"models/pro_generator_128_2.pth\"\n",
    "CHECKPOINT_PRO_DISC = \"models/pro_disc_128_3.pth\"\n",
    "PRO_LR = 5.5e-4\n",
    "FACTORS = [1, 1/2, 1/4, 1/8, 1/16, 1/32]\n",
    "CHANNELS_IMG = 3\n",
    "IN_CHANNELS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Weight scaled Conv2d (Equalized Learning Rate)\n",
    "    Note that input is multiplied rather than changing weights\n",
    "    this will have the same result.\n",
    "\n",
    "    Inspired and looked at:\n",
    "    https://github.com/nvnbny/progressive_growing_of_gans/blob/master/modelUtils.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2\n",
    "    ):\n",
    "        super(WSConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.use_pn = use_pixelnorm\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # initial takes 1x1 -> 4x4\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        )\n",
    "\n",
    "        # Converts n channel image to rgb channel\n",
    "        self.initial_vec_to_rgb = WSConv2d(\n",
    "            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "        self.prog_blocks, self.vec_to_rgb_layers = (\n",
    "            nn.ModuleList([]),\n",
    "            nn.ModuleList([self.initial_vec_to_rgb]),\n",
    "        )\n",
    "\n",
    "        for i in range(\n",
    "            len(FACTORS) - 1\n",
    "        ):  # -1 to prevent index error because of factors[i+1]\n",
    "            conv_in_c = int(in_channels * FACTORS[i])\n",
    "            conv_out_c = int(in_channels * FACTORS[i + 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.vec_to_rgb_layers.append(\n",
    "                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return alpha * generated + (1 - alpha) * upscaled\n",
    "\n",
    "    def forward(self, x, alpha=0.5, steps=5):\n",
    "        out = self.initial(x) # 1x1 to 4x4\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_vec_to_rgb(out)\n",
    "\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        # The number of channels in upscale will stay the same, while\n",
    "        # out which has moved through prog_blocks might change. To ensure\n",
    "        # we can convert both to rgb we use different vec_to_rgb_layers\n",
    "        # (steps-1) and steps for upscaled, out respectively\n",
    "        final_upscaled = self.vec_to_rgb_layers[steps - 1](upscaled)\n",
    "        final_out = self.vec_to_rgb_layers[steps](out)\n",
    "        return torch.tanh(self.fade_in(alpha, final_upscaled, final_out))\n",
    "\n",
    "pro_gen = Generator(\n",
    "    Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG\n",
    ").to(DEVICE)\n",
    "opt_pro_gen = optim.Adam(\n",
    "    pro_gen.parameters(), lr=1e-3, betas=(0.0, 0.99)\n",
    ")\n",
    "\n",
    "load_checkpoint(CHECKPOINT_PRO_GEN, pro_gen)\n",
    "\n",
    "pro_gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.prog_blocks, self.rgb_to_vec_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # here we work back ways from factors because the discriminator\n",
    "        # should be mirrored from the generator. So the first prog_block and\n",
    "        # rgb_to_vec layer we append will work for input size 1024x1024, then 512->256-> etc\n",
    "        for i in range(len(FACTORS) - 1, 0, -1):\n",
    "            conv_in = int(in_channels * FACTORS[i])\n",
    "            conv_out = int(in_channels * FACTORS[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False))\n",
    "            self.rgb_to_vec_layers.append(\n",
    "                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "        # perhaps confusing name \"initial_rgb_to_vec\" this is just the RGBto_vec layer for 4x4 input size\n",
    "        # did this to \"mirror\" the generator initial_rgb_to_vec\n",
    "        self.initial_rgb_to_vec = WSConv2d(\n",
    "            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.rgb_to_vec_layers.append(self.initial_rgb_to_vec)\n",
    "        self.avg_pool = nn.AvgPool2d(\n",
    "            kernel_size=2, stride=2\n",
    "        )  # down sampling using avg pool\n",
    "\n",
    "        # this is the block for 4x4 input size\n",
    "        self.final_block = nn.Sequential(\n",
    "            # +1 to in_channels because we concatenate from MiniBatch std\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(\n",
    "                in_channels, 1, kernel_size=1, padding=0, stride=1\n",
    "            ),\n",
    "            nn.Flatten(),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        batch_statistics = (\n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        )\n",
    "        # we take the std for each example (across all channels, and pixels) then we repeat it\n",
    "        # for a single channel and concatenate it with the image. In this way the discriminator\n",
    "        # will get information about the variation in the batch/image\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha=0.5, steps=5):\n",
    "        # where we should start in the list of prog_blocks, maybe a bit confusing but\n",
    "        # the last is for the 4x4. So example let's say steps=1, then we should start\n",
    "        # at the second to last because input_size will be 8x8. If steps==0 we just\n",
    "        # use the final block\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "\n",
    "        # convert from rgb_to_vec as initial step, this will depend on\n",
    "        # the image size (each will have it's on rgb_to_vec layer)\n",
    "        out = self.leaky(self.rgb_to_vec_layers[cur_step](x))\n",
    "\n",
    "        if steps != 0:# i.e, image is anything other than 4x4\n",
    "            # because prog_blocks might change the channels, for down scale we use rgb_to_vec_layer\n",
    "            # from previous/smaller size which in our case correlates to +1 in the indexing\n",
    "            downscaled = self.leaky(self.rgb_to_vec_layers[cur_step + 1](self.avg_pool(x)))\n",
    "            out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "\n",
    "            # the fade_in is done first between the downscaled and the input\n",
    "            # this is opposite from the generator\n",
    "            out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "            for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "                out = self.prog_blocks[step](out)\n",
    "                out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out)\n",
    "\n",
    "pro_disc = Discriminator(\n",
    "    Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG\n",
    ").to(DEVICE)\n",
    "opt_pro_disc = optim.Adam(\n",
    "    pro_disc.parameters(), lr=1e-3, betas=(0.0, 0.99)\n",
    ")\n",
    "\n",
    "load_checkpoint(CHECKPOINT_PRO_CRITIC, pro_disc, opt_pro_disc, PRO_LR,)\n",
    "\n",
    "for param in pro_disc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "pro_disc.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_WGAN_CRITIC = \"models/wgan_critic_128_2.pth\"\n",
    "CHECKPOINT_WGAN_GEN = \"models/wgan_generator_128_2.pth\"\n",
    "CHECKPOINT_WGAN_DISC = \"models/wgan_discriminator_128_2.pth\"\n",
    "\n",
    "FEATURES_GEN = 16\n",
    "FEATURES_DISC = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise, features_g * 32, 4, 1, 0),  # img: 4\n",
    "            self._block(features_g * 32, features_g * 16, 4, 2, 1),  # img: 8\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 16\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 32\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 64\n",
    "            self._block(features_g * 2, features_g * 1, 4, 2, 1),  # img: 128\n",
    "            nn.Conv2d(\n",
    "                features_g * 1, channels_img, kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 128 x 128\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "wgan_gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(DEVICE)\n",
    "load_checkpoint(CHECKPOINT_WGAN_GEN, wgan_gen)\n",
    "wgan_gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # input: N x channels_img x 128 x 128\n",
    "            nn.Conv2d(channels_img, features_d, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            self._block(features_d * 8, features_d * 16, 4, 2, 1),\n",
    "            self._block(features_d * 16, features_d * 32, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 32, features_d * 32, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features_d * 32, 1, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.Flatten(),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "wgan_disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(DEVICE)\n",
    "load_checkpoint(CHECKPOINT_WGAN_CRITIC, wgan_disc)\n",
    "for param in wgan_disc.parameters():\n",
    "    param.requires_grad = True\n",
    "wgan_disc.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(CHECKPOINT_WGAN_DISC, wgan_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan_disc_test_metrics = evaluate_model(\n",
    "    discriminator=wgan_disc,\n",
    "    generator=wgan_gen,\n",
    "    test_loader=test_loader,\n",
    "    fixed_test_vector=fixed_test_vector,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
